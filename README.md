# Multimodal-Image-Captioning
In this repository, we will implement multimodal Image captioning using PyTorch
For this project, I have used Vision Transformer as the Image Encoder and GPT2 Decoder for textual data. THis is a Multimodal model (since two modalities of data: Image and Text).
Following is the architecture:
![image](https://github.com/lkls1851/Multimodal-Image-Captioning/assets/152661141/4da57409-007d-4956-801e-b8adc47ec075)

Trained this model on Flickr8k dataset. Some results are:
![image](https://github.com/lkls1851/Multimodal-Image-Captioning/assets/152661141/ded51877-5789-44f7-ab09-5ac87ce2db91)

The entire dataset (~8k images) divided into 80:20 ratio for train and validation data.
